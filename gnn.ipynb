{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"},"kaggle":{"accelerator":"none","dataSources":[],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install vmas benchmarl pyvirtualdisplay moviepy\n!apt-get install python3-opengl\nimport pyvirtualdisplay\ndisplay = pyvirtualdisplay.Display(visible=False, size=(1400, 900))\ndisplay.start()","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\n\ndef format_pytorch_version(version):\n  return version.split('+')[0]\n\nTORCH_version = torch.__version__\nTORCH = format_pytorch_version(TORCH_version)\n\ndef format_cuda_version(version):\n  return 'cu' + version.replace('.', '')\n\nCUDA_version = torch.version.cuda\nCUDA = format_cuda_version(CUDA_version)\n\n!pip install torch-cluster -f https://data.pyg.org/whl/torch-{TORCH}+{CUDA}.html\n!pip install torch-geometric","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\n\nimport vmas.simulator.core\nimport vmas.simulator.utils\nfrom vmas.simulator.dynamics.common import Dynamics\n\n\nclass FixedWingKinematicBicycle(Dynamics):\n    def __init__(\n        self,\n        world: vmas.simulator.core.World,\n        width: float,\n        l_f: float,\n        l_r: float,\n        max_steering_angle: float,\n        min_v: float = 0.3,\n        max_v: float = 1.0,\n        integration: str = \"rk4\",\n    ):\n        super().__init__()\n        assert integration in (\n            \"rk4\",\n            \"euler\",\n        ), \"Integration method must be 'euler' or 'rk4'.\"\n        self.width = width\n        self.l_f = l_f\n        self.l_r = l_r\n        self.max_steering_angle = max_steering_angle\n        self.dt = world.dt\n        self.integration = integration\n        self.world = world\n        self.min_v = min_v\n        self.max_v = max_v\n\n    def f(self, state, steering_command, v_command):\n        theta = state[:, 2]  # Yaw angle\n        beta = torch.atan2(\n            torch.tan(steering_command) * self.l_r / (self.l_f + self.l_r),\n            torch.tensor(1, device=self.world.device),\n        )  # [-pi, pi] slip angle\n        dx = v_command * torch.cos(theta + beta)\n        dy = v_command * torch.sin(theta + beta)\n        dtheta = (\n            v_command\n            / (self.l_f + self.l_r)\n            * torch.cos(beta)\n            * torch.tan(steering_command)\n        )\n        return torch.stack((dx, dy, dtheta), dim=1)  # [batch_size,3]\n\n    def euler(self, state, steering_command, v_command):\n        # Calculate the change in state using Euler's method\n        # For Euler's method, see https://math.libretexts.org/Bookshelves/Calculus/Book%3A_Active_Calculus_(Boelkins_et_al.)/07%3A_Differential_Equations/7.03%3A_Euler's_Method (the full link may not be recognized properly, please copy and paste in your browser)\n        return self.dt * self.f(state, steering_command, v_command)\n\n    def runge_kutta(self, state, steering_command, v_command):\n        # Calculate the change in state using fourth-order Runge-Kutta method\n        # For Runge-Kutta method, see https://math.libretexts.org/Courses/Monroe_Community_College/MTH_225_Differential_Equations/3%3A_Numerical_Methods/3.3%3A_The_Runge-Kutta_Method\n        k1 = self.f(state, steering_command, v_command)\n        k2 = self.f(state + self.dt * k1 / 2, steering_command, v_command)\n        k3 = self.f(state + self.dt * k2 / 2, steering_command, v_command)\n        k4 = self.f(state + self.dt * k3, steering_command, v_command)\n        return (self.dt / 6) * (k1 + 2 * k2 + 2 * k3 + k4)\n\n    @property\n    def needed_action_size(self) -> int:\n        return 2\n\n    def process_action(self):\n        # Extracts the velocity and steering angle from the agent's actions and convert them to physical force and torque\n        v_command = self.agent.action.u[:, 0]\n        # The only change we make:\n        v_command = torch.clamp(\n            v_command, self.min_v, self.max_v\n        )\n\n        steering_command = self.agent.action.u[:, 1]\n        # Ensure steering angle is within bounds\n        steering_command = torch.clamp(\n            steering_command, -self.max_steering_angle, self.max_steering_angle\n        )\n\n        # Current state of the agent\n        state = torch.cat((self.agent.state.pos, self.agent.state.rot), dim=1)\n\n        v_cur_x = self.agent.state.vel[:, 0]  # Current velocity in x-direction\n        v_cur_y = self.agent.state.vel[:, 1]  # Current velocity in y-direction\n        v_cur_angular = self.agent.state.ang_vel[:, 0]  # Current angular velocity\n\n        # Select the integration method to calculate the change in state\n        if self.integration == \"euler\":\n            delta_state = self.euler(state, steering_command, v_command)\n        else:\n            delta_state = self.runge_kutta(state, steering_command, v_command)\n\n        # Calculate the accelerations required to achieve the change in state.\n        acceleration_x = (delta_state[:, 0] - v_cur_x * self.dt) / self.dt**2\n        acceleration_y = (delta_state[:, 1] - v_cur_y * self.dt) / self.dt**2\n        acceleration_angular = (\n            delta_state[:, 2] - v_cur_angular * self.dt\n        ) / self.dt**2\n\n        # Calculate the forces required for the linear accelerations\n        force_x = self.agent.mass * acceleration_x\n        force_y = self.agent.mass * acceleration_y\n\n        # Calculate the torque required for the angular acceleration\n        torque = self.agent.moment_of_inertia * acceleration_angular\n\n        # Update the physical force and torque required for the user inputs\n        self.agent.state.force[:, vmas.simulator.utils.X] = force_x\n        self.agent.state.force[:, vmas.simulator.utils.Y] = force_y\n        self.agent.state.torque = torque.unsqueeze(-1)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import typing\nfrom typing import Callable, Dict, List\n\nimport torch\nfrom torch import Tensor\n\nfrom vmas import render_interactively\nfrom vmas.simulator.core import Agent, Entity, Landmark, Box, Sphere, World\nfrom vmas.simulator.scenario import BaseScenario\nfrom vmas.simulator.sensors import Lidar\nfrom vmas.simulator.utils import Color, ScenarioUtils, X, Y\nfrom vmas.simulator.dynamics.kinematic_bicycle import KinematicBicycle\n\nif typing.TYPE_CHECKING:\n    from vmas.simulator.rendering import Geom\n\n\nclass FWDiscoveryScenario(BaseScenario):\n    def make_world(self, batch_dim: int, device: torch.device, **kwargs):\n        self.n_agents = kwargs.pop(\"n_agents\", 5)\n        self.n_targets = kwargs.pop(\"n_targets\", 7)\n        self.x_semidim = kwargs.pop(\"x_semidim\", 1)\n        self.y_semidim = kwargs.pop(\"y_semidim\", 1)\n        self._min_dist_between_entities = kwargs.pop(\"min_dist_between_entities\", 0.2)\n        self._lidar_range = kwargs.pop(\"lidar_range\", 0.35)\n        self._covering_range = kwargs.pop(\"covering_range\", 0.25)\n\n        self.use_agent_lidar = kwargs.pop(\"use_agent_lidar\", False)\n        self.n_lidar_rays_entities = kwargs.pop(\"n_lidar_rays_entities\", 15)\n        self.n_lidar_rays_agents = kwargs.pop(\"n_lidar_rays_agents\", 12)\n\n        self._agents_per_target = kwargs.pop(\"agents_per_target\", 2)\n        self.targets_respawn = kwargs.pop(\"targets_respawn\", True)\n        self.shared_reward = kwargs.pop(\"shared_reward\", False)\n\n        self.agent_collision_penalty = kwargs.pop(\"agent_collision_penalty\", 0)\n        self.covering_rew_coeff = kwargs.pop(\"covering_rew_coeff\", 1.0)\n        self.time_penalty = kwargs.pop(\"time_penalty\", 0)\n        self.render_action = kwargs.pop(\"render_action\", False) # Modification\n        ScenarioUtils.check_kwargs_consumed(kwargs)\n\n        self._comms_range = self._lidar_range\n        self.min_collision_distance = 0.005\n        self.agent_radius = 0.05\n        self.target_radius = self.agent_radius\n\n        self.viewer_zoom = 1\n        self.target_color = Color.GREEN\n\n        # Make world\n        world = World(\n            batch_dim,\n            device,\n            x_semidim=self.x_semidim,\n            y_semidim=self.y_semidim,\n            collision_force=500,\n            substeps=2,\n            drag=0.25,\n        )\n\n        # Add agents\n        entity_filter_agents: Callable[[Entity], bool] = lambda e: e.name.startswith(\n            \"agent\"\n        )\n        entity_filter_targets: Callable[[Entity], bool] = lambda e: e.name.startswith(\n            \"target\"\n        )\n        _max_steering_angle = torch.pi/4\n        for i in range(self.n_agents):\n            # Constraint: all agents have same action range and multiplier\n            agent = Agent(\n                name=f\"agent_{i}\",\n                collide=True,\n                color=Color.ORANGE, # Modification (not important)\n                shape=Box(length=self.agent_radius * 2, width=self.agent_radius),\n                sensors=(\n                    [\n                        Lidar(\n                            world,\n                            n_rays=self.n_lidar_rays_entities,\n                            max_range=self._lidar_range,\n                            entity_filter=entity_filter_targets,\n                            render_color=Color.GREEN,\n                        )\n                    ]\n                    + (\n                        [\n                            Lidar(\n                                world,\n                                angle_start=0.05,\n                                angle_end=2 * torch.pi + 0.05,\n                                n_rays=self.n_lidar_rays_agents,\n                                max_range=self._lidar_range,\n                                entity_filter=entity_filter_agents,\n                                render_color=Color.BLUE,\n                            )\n                        ]\n                        if self.use_agent_lidar\n                        else []\n                    )\n                ),\n                dynamics=FixedWingKinematicBicycle(\n                    world,\n                    width=self.agent_radius,\n                    l_f=self.agent_radius,\n                    l_r=self.agent_radius,\n                    max_steering_angle=_max_steering_angle\n                ), # Modification\n                render_action=self.render_action # Modification\n            )\n            agent.collision_rew = torch.zeros(batch_dim, device=device)\n            agent.covering_reward = agent.collision_rew.clone()\n            world.add_agent(agent)\n\n        self._targets = []\n        for i in range(self.n_targets):\n            target = Landmark(\n                name=f\"target_{i}\",\n                collide=True,\n                movable=False,\n                shape=Sphere(radius=self.target_radius),\n                color=self.target_color,\n            )\n            world.add_landmark(target)\n            self._targets.append(target)\n\n        self.covered_targets = torch.zeros(batch_dim, self.n_targets, device=device)\n        self.shared_covering_rew = torch.zeros(batch_dim, device=device)\n\n        return world\n\n    def reset_world_at(self, env_index: int = None):\n        placable_entities = self._targets[: self.n_targets] + self.world.agents\n        if env_index is None:\n            self.all_time_covered_targets = torch.full(\n                (self.world.batch_dim, self.n_targets),\n                False,\n                device=self.world.device,\n            )\n        else:\n            self.all_time_covered_targets[env_index] = False\n        ScenarioUtils.spawn_entities_randomly(\n            entities=placable_entities,\n            world=self.world,\n            env_index=env_index,\n            min_dist_between_entities=self._min_dist_between_entities,\n            x_bounds=(-self.world.x_semidim, self.world.x_semidim),\n            y_bounds=(-self.world.y_semidim, self.world.y_semidim),\n        )\n        for target in self._targets[self.n_targets :]:\n            target.set_pos(self.get_outside_pos(env_index), batch_index=env_index)\n\n    def reward(self, agent: Agent):\n        is_first = agent == self.world.agents[0]\n        is_last = agent == self.world.agents[-1]\n\n        if is_first:\n            self.time_rew = torch.full(\n                (self.world.batch_dim,),\n                self.time_penalty,\n                device=self.world.device,\n            )\n            self.agents_pos = torch.stack(\n                [a.state.pos for a in self.world.agents], dim=1\n            )\n            self.targets_pos = torch.stack([t.state.pos for t in self._targets], dim=1)\n            self.agents_targets_dists = torch.cdist(self.agents_pos, self.targets_pos)\n            self.agents_per_target = torch.sum(\n                (self.agents_targets_dists < self._covering_range).type(torch.int),\n                dim=1,\n            )\n            self.covered_targets = self.agents_per_target >= self._agents_per_target\n\n            self.shared_covering_rew[:] = 0\n            for a in self.world.agents:\n                self.shared_covering_rew += self.agent_reward(a)\n            self.shared_covering_rew[self.shared_covering_rew != 0] /= 2\n\n        # Avoid collisions with each other\n        agent.collision_rew[:] = 0\n        for a in self.world.agents:\n            if a != agent:\n                agent.collision_rew[\n                    self.world.get_distance(a, agent) < self.min_collision_distance\n                ] += self.agent_collision_penalty\n\n        if is_last:\n            if self.targets_respawn:\n                occupied_positions_agents = [self.agents_pos]\n                for i, target in enumerate(self._targets):\n                    occupied_positions_targets = [\n                        o.state.pos.unsqueeze(1)\n                        for o in self._targets\n                        if o is not target\n                    ]\n                    occupied_positions = torch.cat(\n                        occupied_positions_agents + occupied_positions_targets,\n                        dim=1,\n                    )\n                    pos = ScenarioUtils.find_random_pos_for_entity(\n                        occupied_positions,\n                        env_index=None,\n                        world=self.world,\n                        min_dist_between_entities=self._min_dist_between_entities,\n                        x_bounds=(-self.world.x_semidim, self.world.x_semidim),\n                        y_bounds=(-self.world.y_semidim, self.world.y_semidim),\n                    )\n\n                    target.state.pos[self.covered_targets[:, i]] = pos[\n                        self.covered_targets[:, i]\n                    ].squeeze(1)\n            else:\n                self.all_time_covered_targets += self.covered_targets\n                for i, target in enumerate(self._targets):\n                    target.state.pos[self.covered_targets[:, i]] = self.get_outside_pos(\n                        None\n                    )[self.covered_targets[:, i]]\n        covering_rew = (\n            agent.covering_reward\n            if not self.shared_reward\n            else self.shared_covering_rew\n        )\n\n        return agent.collision_rew + covering_rew + self.time_rew\n\n    def get_outside_pos(self, env_index):\n        return torch.empty(\n            (\n                (1, self.world.dim_p)\n                if env_index is not None\n                else (self.world.batch_dim, self.world.dim_p)\n            ),\n            device=self.world.device,\n        ).uniform_(-1000 * self.world.x_semidim, -10 * self.world.x_semidim)\n\n    def agent_reward(self, agent):\n        agent_index = self.world.agents.index(agent)\n\n        agent.covering_reward[:] = 0\n        targets_covered_by_agent = (\n            self.agents_targets_dists[:, agent_index] < self._covering_range\n        )\n        num_covered_targets_covered_by_agent = (\n            targets_covered_by_agent * self.covered_targets\n        ).sum(dim=-1)\n        agent.covering_reward += (\n            num_covered_targets_covered_by_agent * self.covering_rew_coeff\n        )\n        return agent.covering_reward\n\n    def observation(self, agent: Agent):\n        lidar_1_measures = agent.sensors[0].measure()\n        obs = {\"obs\" : torch.cat(\n            [lidar_1_measures]\n            + ([agent.sensors[1].measure()] if self.use_agent_lidar else []),\n            dim=-1),\n                \"pos\" : agent.state.pos,\n                \"vel\" : agent.state.vel\n        }\n        if isinstance(agent.dynamics, KinematicBicycle) or isinstance(agent.dynamics, FixedWingKinematicBicycle):\n            obs.update({\n                \"rot\": agent.state.rot,\n                \"ang_vel\": agent.state.ang_vel\n            })\n        return obs\n\n    def info(self, agent: Agent) -> Dict[str, Tensor]:\n        info = {\n            \"covering_reward\": (\n                agent.covering_reward\n                if not self.shared_reward\n                else self.shared_covering_rew\n            ),\n            \"collision_rew\": agent.collision_rew,\n            \"targets_covered\": self.covered_targets.sum(-1),\n        }\n        return info\n\n    def done(self):\n        return self.all_time_covered_targets.all(dim=-1)\n\n    def extra_render(self, env_index: int = 0) -> \"List[Geom]\":\n        from vmas.simulator import rendering\n\n        geoms: List[Geom] = []\n        # Target ranges\n        for target in self._targets:\n            range_circle = rendering.make_circle(self._covering_range, filled=False)\n            xform = rendering.Transform()\n            xform.set_translation(*target.state.pos[env_index])\n            range_circle.add_attr(xform)\n            range_circle.set_color(*self.target_color.value)\n            geoms.append(range_circle)\n        # Communication lines\n        for i, agent1 in enumerate(self.world.agents):\n            for j, agent2 in enumerate(self.world.agents):\n                if j <= i:\n                    continue\n                agent_dist = torch.linalg.vector_norm(\n                    agent1.state.pos - agent2.state.pos, dim=-1\n                )\n                if agent_dist[env_index] <= self._comms_range:\n                    color = Color.BLACK.value\n                    line = rendering.Line(\n                        (agent1.state.pos[env_index]),\n                        (agent2.state.pos[env_index]),\n                        width=1,\n                    )\n                    xform = rendering.Transform()\n                    line.add_attr(xform)\n                    line.set_color(*color)\n                    geoms.append(line)\n\n        return geoms","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import copy\nfrom typing import Callable, Optional\nfrom benchmarl.environments import VmasTask\nfrom benchmarl.utils import DEVICE_TYPING\nfrom torchrl.envs import EnvBase, VmasEnv\n\ndef get_env_fun(\n    self,\n    num_envs: int,\n    continuous_actions: bool,\n    seed: Optional[int],\n    device: DEVICE_TYPING):\n  config = copy.deepcopy(self.config)\n  if (hasattr(self, \"name\") and self.name == \"NAVIGATION\") or (\n      self is VmasTask.NAVIGATION\n  ):  \n      scenario = FWDiscoveryScenario()  \n  else:\n      scenario = self.name.lower()\n  return lambda: VmasEnv(\n      scenario=scenario,\n      num_envs=num_envs,\n      continuous_actions=continuous_actions,\n      seed=seed,\n      device=device,\n      categorical_actions=True,\n      **config)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"try:\n    from benchmarl.environments import VmasClass\n    VmasClass.get_env_fun = get_env_fun\nexcept ImportError:\n    print(\"Import Error\")\n    VmasTask.get_env_fun = get_env_fun","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import wandb\nimport os\nfrom kaggle_secrets import UserSecretsClient\nsecrets = UserSecretsClient()\nos.environ[\"WANDB_API_KEY\"] = secrets.get_secret(\"WANDB_API_KEY\")\nos.environ[\"WANDB_MODE\"] = \"online\"\nwandb.login()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from benchmarl.experiment import ExperimentConfig\n\nexperiment_config = ExperimentConfig.get_from_yaml() # We start by loading the defaults\n\nexperiment_config.sampling_device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nexperiment_config.train_device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\nexperiment_config.max_n_frames = 20_000_000\nexperiment_config.gamma = 0.99\nexperiment_config.on_policy_collected_frames_per_batch = 100_000\nexperiment_config.on_policy_n_envs_per_worker = 1000\nexperiment_config.on_policy_n_minibatch_iters = 45\nexperiment_config.on_policy_minibatch_size = 4096\nexperiment_config.evaluation = True\nexperiment_config.render = True\nexperiment_config.share_policy_params = True\nexperiment_config.evaluation_interval = 200_000\nexperiment_config.evaluation_episodes = 20\nexperiment_config.loggers = [\"wandb\"]\n\ntask = VmasTask.NAVIGATION.get_from_yaml()\n\ntask.config = {\n    \"max_steps\" : 100,\n    \"n_agents\" : 4,\n    \"shared_reward\" : False,\n    \"x_semidim\" : 1,\n    \"y_semidim\" : 1,\n    \"render_action\" : True,\n    \"agents_per_target\" : 1,\n    \"use_agent_lidar\" : False,\n    \"agent_collision_penalty\" : -1,\n    \"time_penalty\" : -0.01\n}\n\nfrom benchmarl.algorithms import MappoConfig\nmappo_algorithm_config = MappoConfig.get_from_yaml()\nmappo_algorithm_config = MappoConfig(\n        share_param_critic=True,\n        clip_epsilon=0.2,\n        entropy_coef=0.001,\n        critic_coef=1,\n        loss_critic_type=\"l2\",\n        lmbda=0.9,\n        scale_mapping=\"biased_softplus_1.0\",\n        use_tanh_normal=True,\n        minibatch_advantage=False,\n    )\n\nfrom benchmarl.models import MlpConfig\ncritic_model_config = MlpConfig(\n        num_cells=[256, 256], \n        layer_class=torch.nn.Linear,\n        activation_class=torch.nn.SiLU,\n)\n\nfrom benchmarl.models import GnnConfig, SequenceModelConfig\nimport torch_geometric\n\ncomms_radius = 2\ngnn_config = GnnConfig(\n    topology=\"from_pos\", \n    edge_radius=comms_radius,\n    self_loops=False,\n    gnn_class=torch_geometric.nn.conv.GATv2Conv,\n    gnn_kwargs={\"add_self_loops\": False, \"residual\": True}, \n    position_key=\"pos\",\n    pos_features=2,\n    exclude_pos_from_node_features=True, \nmlp_config = MlpConfig.get_from_yaml()\n\nmodel_config_gnn = SequenceModelConfig(model_configs=[gnn_config, mlp_config], intermediate_sizes=[256])\n\nfrom benchmarl.experiment import Experiment\nexperiment = Experiment(\n    task=task,\n    algorithm_config=mappo_algorithm_config,\n    model_config=model_config_gnn,\n    critic_model_config=critic_model_config,\n    seed=1337,\n    config=experiment_config,\n)\nexperiment.run()","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}
